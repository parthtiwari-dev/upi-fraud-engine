# ğŸš¨ Real-Time UPI Fraud Detection System

> A production-grade machine learning system that simulates how real fintech companies detect fraud â€” built with strict temporal correctness, alert budgets, and end-to-end deployment.

**This is not a Kaggle project.**
It is a full ML system: data â†’ features â†’ models â†’ business logic â†’ API â†’ deployment.

---

## âš¡ One-Minute Overview

**Problem:**
At transaction time **T**, using only past data (labels arrive late), decide whether to raise a fraud alert under a fixed daily alert budget â€” in under 500ms.

**Solution:**
A two-stage fraud detection system with point-in-time features, leakage-free training, real-time inference, and production deployment.

**Core Capabilities:**
- Realistic UPI transaction simulation (1.1M+ transactions)
- Batch + streaming pipelines with parity checks
- Point-in-time feature engineering (no future leakage)
- Two-stage ML models (Isolation Forest + XGBoost)
- Backtesting under alert budget constraints
- FastAPI backend + Streamlit UI + Docker deployment

---

## ğŸ§  System Architecture (Big Picture)

```
User / Client
    â”‚
    â–¼
Streamlit UI (Frontend)
    â”‚
    â–¼
FastAPI Scoring Service (Docker, Render)
    â”‚
    â”œâ”€ Online Feature Store (Stateful)
    â”œâ”€ Stage 1: Isolation Forest (Anomaly Detection)
    â”œâ”€ Stage 2: XGBoost (Fraud Classification)
    â””â”€ Alert Policy Engine (0.5% Budget)
    â”‚
    â–¼
Fraud Probability + Alert Decision + Business Metrics
```

---

## ğŸ—ï¸ End-to-End ML Pipeline

```
PHASE 1 â”€ Data Generation (UPI Simulation)
    â†“
PHASE 2 â”€ Ingestion (Batch + Streaming)
    â†“
PHASE 3 â”€ Data Validation (Great Expectations)
    â†“
PHASE 4 â”€ Feature Engineering (Point-in-Time Safe)
    â†“
PHASE 5 â”€ Model Training & Leakage Audit
    â†“
PHASE 6 â”€ Backtesting & Business Evaluation
    â†“
PHASE 7 â”€ Real-Time API
    â†“
PHASE 8 â”€ Production Deployment
```

---

## ğŸ“Š Key Results

### Dataset
- Transactions: **1,097,231**
- Fraud rate: **3.6% (labeled data)**
- Features: **482 production-safe features**

### Model Performance (Leakage-Free)
- ROC-AUC: **0.8918**
- Precision @ 0.5% alert budget: **~90%+**
- Recall @ 0.5% alert budget: **~12%**

### Production Metrics
- Latency: **~233ms avg (<500ms target)**
- Deployment: Render + Streamlit Cloud
- Architecture: Docker + FastAPI + Stateful Features

---

# ğŸ§© Phase-by-Phase System Design

---

## PHASE 1 â€” Realistic UPI Data Generation

### Objective
Simulate a real UPI ecosystem with realistic fraud patterns.

### Fraud Patterns
- Device rings
- Velocity bursts
- Time anomalies
- Label delays (fraud discovered hours/days later)

### Pipeline
```
IEEE-CIS Data â†’ UPI Schema Mapping â†’ Fraud Injection â†’ Validation â†’ DuckDB
```

Outcome: A synthetic but realistic fintech dataset suitable for system-level ML design.

---

## PHASE 2 â€” Ingestion Pipeline (Batch + Streaming)

### Problem Solved
**Trainingâ€“serving skew** â€” the most common production ML failure.

### Architecture
```
DuckDB
  â”œâ”€ Batch Loader (Training)
  â””â”€ Streaming Simulator (Serving)
         â†“
   Consistency Check (Identical Outputs)
```

Guarantee: Offline and online pipelines see identical data formats and semantics.

---

## PHASE 3 â€” Data Validation & Temporal Guarantees

### Tooling
- Great Expectations

### Enforced Constraints
- Schema correctness
- Type safety
- Unique IDs
- Temporal causality (no future data)
- Label delay constraints

Key Insight:
> If your data is temporally wrong, your model is meaningless.

---

## PHASE 4 â€” Feature Engineering (Point-in-Time Safe)

### Feature Families
1. Velocity Features (5min, 1h, 24h)
2. Graph Features (device â†” users)
3. Risk History (label-aware)

### Core Rule
For every transaction T:
> Features must use only data from time < T.

### Output
- 487 total features
- Strictly leakage-free feature store

---

## PHASE 5 â€” Model Training & Leakage Audit

### Two-Stage Architecture

```
Stage 1: Isolation Forest (unsupervised anomalies)
Stage 2: XGBoost (supervised fraud classification)
```

### Critical Discovery
A synthetic column (`fraud_pattern`) caused label leakage.

```
AUC with leakage: 0.9106
AUC without leakage: 0.8918
```

Decision:
Deploy the leakage-free model despite lower metrics â€” prioritizing correctness over vanity scores.

---

## PHASE 6 â€” Backtesting & Business Evaluation

### Why Standard ML Metrics Fail
Real systems operate under operational constraints.

### Implemented
- Day-by-day replay
- Alert budget enforcement (0.5%)
- Costâ€“benefit analysis
- ROI estimation

Example Insight:
```
High accuracy â‰  useful system
Useful system = accuracy under budget constraints
```

---

## PHASE 7 â€” Real-Time Fraud Detection API

### Architecture

```
FastAPI API
   â†“
Online Feature Store (stateful)
   â†“
Two-Stage Model
   â†“
Alert Policy Engine
```

Capabilities:
- Real-time scoring (<500ms)
- Stateful feature updates
- Business-layer decision logic
- Production-ready inference pipeline

---

## PHASE 8 â€” Production Deployment

### Stack
- Backend: FastAPI + Docker + Render
- Frontend: Streamlit Cloud

### Live System Flow

```
User â†’ Streamlit UI â†’ FastAPI API â†’ Model â†’ Decision
```

---

# ğŸ§± Repository Structure

```
upi-fraud-engine/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/            # FastAPI backend
â”‚   â”œâ”€â”€ ingestion/      # Batch & streaming loaders
â”‚   â”œâ”€â”€ validation/     # Great Expectations
â”‚   â”œâ”€â”€ features/       # Feature engineering
â”‚   â”œâ”€â”€ evaluation/     # Backtesting & metrics
â”‚   â””â”€â”€ inference/      # Model inference
â”œâ”€â”€ models/             # Trained models & encoders
â”œâ”€â”€ data_generation/    # Synthetic UPI data pipeline
â”œâ”€â”€ evaluation/         # Reports & visualizations
â”œâ”€â”€ docs/               # Phase-wise documentation
â”œâ”€â”€ notebooks/          # Experiments
â”œâ”€â”€ dockerfile          # Deployment
â”œâ”€â”€ app.py              # Streamlit UI
â””â”€â”€ README.md           # (this file)
```

---

# ğŸ¯ Core Design Principles

### 1) Temporal Correctness
No future information is used in training or inference.

### 2) Trainingâ€“Serving Parity
Batch and streaming pipelines are provably identical.

### 3) Business-First Evaluation
Metrics reflect operational constraints, not just accuracy.

### 4) Production Realism
System design mirrors real fintech fraud engines.

---

# ğŸ’¡ Why This Project Is Different

Most ML projects answer:
> â€œCan I train a model?â€

This project answers:
> â€œCan I build a system that would actually work in production?â€

It demonstrates:
- ML engineering + data engineering + backend integration
- real-world fraud constraints (latency, budget, label delay)
- system-level thinking beyond algorithms

---

# ğŸš€ Future Extensions

- Kafka-based streaming pipeline
- Redis-backed online feature store
- Automated model retraining
- Drift detection & monitoring
- Online A/B testing
- Real UPI-scale simulation

---

# ğŸ‘¤ Author

**Parth Tiwari**  
Aspiring ML / AI Engineer focused on building production-grade ML systems.

---

# ğŸ§  Final Thought

Accuracy is easy.
Correctness is hard.
Production realism is harder.

This project was built to solve the hardest one.

